{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from Scripts.dirty_accuracy import injection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helper functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Naive Bayes Classification implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "outputs": [],
   "source": [
    "def NBClassification(X_train, y_train, X_test, y_test):\n",
    "    nb = GaussianNB()\n",
    "    nb.fit(X_train, y_train)\n",
    "    training_score = nb.score(X_train, y_train)\n",
    "    test_score = nb.score(X_test, y_test)\n",
    "    return [training_score, test_score]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Decision Tree Classification implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "outputs": [],
   "source": [
    "def DTClassification(X_train, y_train, X_test, y_test):\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    dtc.fit(X_train, y_train)\n",
    "    training_score = dtc.score(X_train, y_train)\n",
    "    test_score = dtc.score(X_test, y_test)\n",
    "    return [training_score, test_score]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Classification implementation that turns all categorical columns into numerical values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "outputs": [],
   "source": [
    "def DCmap(df):\n",
    "    dataset = df.copy()\n",
    "    for col in dataset:\n",
    "        if dataset[col].dtype == object:\n",
    "            to_replace = dataset[col].unique()\n",
    "            value = []\n",
    "            for i in range (len(to_replace)):\n",
    "                value.append(i)\n",
    "            dataset[col] = dataset[col].replace(to_replace=to_replace, value=value)\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Print scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "outputs": [],
   "source": [
    "def printScores(scores):\n",
    "    for score in scores:\n",
    "        print(\"Dataset dirty at: \", score['dirty'], \"%\")\n",
    "        print(score['scores'][0],':', score['scores'][1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "IRQ detection and correction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "outputs": [],
   "source": [
    "def IRQ(data):\n",
    "    Q1, Q3 = np.percentile(data, [25, 75])\n",
    "    IQR = Q3 - Q1\n",
    "    lower_range = Q1 - (1.5 * IQR)\n",
    "    upper_range = Q3 + (1.5 * IQR)\n",
    "    outliers = data[((data < lower_range) | (data > upper_range))]\n",
    "    for i in range(len(data)):\n",
    "        if data[i] in outliers:\n",
    "            data[i] = (lower_range + upper_range)/2\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "DBSCAN detection and correction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "def KMeansDetectionColumn(data, col):\n",
    "    km = KMeans(n_clusters=2)\n",
    "    clusters = km.fit_predict(data)\n",
    "    centroids = km.cluster_centers_\n",
    "    points = np.empty((0,len(data[col])), float)\n",
    "    distances = np.empty((0,len(data[col])), float)\n",
    "    for i, center_elem in enumerate(centroids):\n",
    "        distances = np.append(distances, cdist([center_elem],data[clusters == i], 'euclidean'))\n",
    "        points = np.append(points, data[clusters == i], axis=0)\n",
    "    percentile = 80\n",
    "    outliers = points[np.where(distances > np.percentile(distances, percentile))]\n",
    "    print(outliers)\n",
    "    return data[col]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "outputs": [],
   "source": [
    "def KMeansDetection(data):\n",
    "    columns = [\"Sex\",\"Housing\",\"Saving_accounts\",\"Checking_account\",\"Purpose\"]\n",
    "    for col in data.columns:\n",
    "        if col not in columns:\n",
    "            columns.append(col)\n",
    "            data[col] = KMeansDetectionColumn(data[columns], col)\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pipeline implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read the dataset from CSV file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "outputs": [],
   "source": [
    "german = pd.read_csv(\"datasets/german.csv\", sep=',')\n",
    "ss = StandardScaler()\n",
    "literal_cols = [\"Sex\",\"Housing\",\"Saving_accounts\",\"Checking_account\",\"Purpose\", \"Risk\"]\n",
    "numerical_cols = [\"Age\", \"Job\", \"Credit_amount\", \"Duration\"]\n",
    "german_str = german[literal_cols]\n",
    "german_num = german[numerical_cols]\n",
    "german_num = pd.DataFrame(ss.fit_transform(german_num), columns=numerical_cols)\n",
    "german = pd.concat([german_str, german_num], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perform the injections of outliers\n",
    "and append the original clean dataset to the list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved german-accuracy50%\n",
      "saved german-accuracy60%\n",
      "saved german-accuracy70%\n",
      "saved german-accuracy80%\n",
      "saved german-accuracy90%\n"
     ]
    }
   ],
   "source": [
    "german_list = injection(df_pandas=german, seed=10, name='german', name_class='Risk')\n",
    "german_list.append(german)\n",
    "for i in range(len(german_list)):\n",
    "    german_list[i] = DCmap(german_list[i])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Reverse the list on order to have\n",
    "0 - Original Dataset\n",
    "1 - 10% Dirty dataset\n",
    "2 - 20% Dirty dataset\n",
    "3 - 30% Dirty dataset\n",
    "4 - 40% Dirty dataset\n",
    "5 - 50% Dirty dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "outputs": [],
   "source": [
    "german_list.reverse()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Functions to be repeated for each dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "outputs": [],
   "source": [
    "def computeScoresDT(dataframe_array):\n",
    "    y = german['Risk']\n",
    "    scores = []\n",
    "    i = 0\n",
    "    for df in dataframe_array:\n",
    "        X = df.drop('Risk', axis=1, errors='ignore')\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "        sc = DTClassification(X_train, y_train, X_test, y_test)\n",
    "        scores.append({'dirty': i, 'scores': sc})\n",
    "        i = i + 10\n",
    "    return scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "outputs": [],
   "source": [
    "def computeScoresNB(dataframe_array):\n",
    "    y = german['Risk']\n",
    "    scores = []\n",
    "    i = 0\n",
    "    for df in dataframe_array:\n",
    "        X = df.drop('Risk', axis=1, errors='ignore')\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "        sc = NBClassification(X_train, y_train, X_test, y_test)\n",
    "        scores.append({'dirty': i, 'scores': sc})\n",
    "        i = i + 10\n",
    "    return scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "outputs": [],
   "source": [
    "def cleanDatasetsIRQ(datasets):\n",
    "    cleaned_list = []\n",
    "    for df in datasets:\n",
    "        cleaned = df.copy()\n",
    "        for col in cleaned.columns:\n",
    "            if col not in [\"Sex\",\"Housing\",\"Saving_accounts\",\"Checking_account\",\"Purpose\", \"Risk\"]:\n",
    "                cleaned[col] = IRQ(cleaned[col].values)\n",
    "        cleaned_list.append(cleaned)\n",
    "    return cleaned_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "outputs": [],
   "source": [
    "def cleanDatasetKMeans(datasets):\n",
    "    cleaned_list = []\n",
    "    for df in datasets:\n",
    "        cleaned = df.copy()\n",
    "        cleaned = cleaned.drop('Risk', axis=1)\n",
    "        cleaned = KMeansDetection(cleaned)\n",
    "        cleaned_list.append(cleaned)\n",
    "    return cleaned_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compute scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dirty at:  0 %\n",
      "1.0 : 0.6466666666666666\n",
      "Dataset dirty at:  10 %\n",
      "1.0 : 0.6533333333333333\n",
      "Dataset dirty at:  20 %\n",
      "1.0 : 0.6\n",
      "Dataset dirty at:  30 %\n",
      "1.0 : 0.58\n",
      "Dataset dirty at:  40 %\n",
      "1.0 : 0.5966666666666667\n",
      "Dataset dirty at:  50 %\n",
      "1.0 : 0.6066666666666667\n"
     ]
    }
   ],
   "source": [
    "DTscores = computeScoresDT(german_list)\n",
    "printScores(DTscores)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dirty at:  0 %\n",
      "0.6885714285714286 : 0.7266666666666667\n",
      "Dataset dirty at:  10 %\n",
      "0.6657142857142857 : 0.6633333333333333\n",
      "Dataset dirty at:  20 %\n",
      "0.6914285714285714 : 0.6566666666666666\n",
      "Dataset dirty at:  30 %\n",
      "0.6814285714285714 : 0.6366666666666667\n",
      "Dataset dirty at:  40 %\n",
      "0.6942857142857143 : 0.67\n",
      "Dataset dirty at:  50 %\n",
      "0.6828571428571428 : 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "NBscore = computeScoresNB(german_list)\n",
    "printScores(NBscore)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Find column by column outliers using ZScore and DBSCAN and replace them with mean value"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 1000 and the array at index 1 has size 6",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [504], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m cleaned_zs_lists \u001B[38;5;241m=\u001B[39m cleanDatasetsIRQ(german_list)\n\u001B[0;32m----> 2\u001B[0m cleaned_km_list \u001B[38;5;241m=\u001B[39m cleanDatasetKMeans(german_list)\n",
      "Cell \u001B[0;32mIn [501], line 6\u001B[0m, in \u001B[0;36mcleanDatasetKMeans\u001B[0;34m(datasets)\u001B[0m\n\u001B[1;32m      4\u001B[0m     cleaned \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[1;32m      5\u001B[0m     cleaned \u001B[38;5;241m=\u001B[39m cleaned\u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRisk\u001B[39m\u001B[38;5;124m'\u001B[39m, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m----> 6\u001B[0m     cleaned \u001B[38;5;241m=\u001B[39m \u001B[43mKMeansDetection\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcleaned\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m     cleaned_list\u001B[38;5;241m.\u001B[39mappend(cleaned)\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cleaned_list\n",
      "Cell \u001B[0;32mIn [494], line 6\u001B[0m, in \u001B[0;36mKMeansDetection\u001B[0;34m(data)\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m col \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m columns:\n\u001B[1;32m      5\u001B[0m         columns\u001B[38;5;241m.\u001B[39mappend(col)\n\u001B[0;32m----> 6\u001B[0m         data[col] \u001B[38;5;241m=\u001B[39m \u001B[43mKMeansDetectionColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "Cell \u001B[0;32mIn [493], line 12\u001B[0m, in \u001B[0;36mKMeansDetectionColumn\u001B[0;34m(data, col)\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, center_elem \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(centroids):\n\u001B[1;32m     11\u001B[0m     distances \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mappend(distances, cdist([center_elem],data[clusters \u001B[38;5;241m==\u001B[39m i], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124meuclidean\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m---> 12\u001B[0m     points \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mappend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpoints\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[43mclusters\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m percentile \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m80\u001B[39m\n\u001B[1;32m     14\u001B[0m outliers \u001B[38;5;241m=\u001B[39m points[np\u001B[38;5;241m.\u001B[39mwhere(distances \u001B[38;5;241m>\u001B[39m np\u001B[38;5;241m.\u001B[39mpercentile(distances, percentile))]\n",
      "File \u001B[0;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36mappend\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/site-packages/numpy/lib/function_base.py:5444\u001B[0m, in \u001B[0;36mappend\u001B[0;34m(arr, values, axis)\u001B[0m\n\u001B[1;32m   5442\u001B[0m     values \u001B[38;5;241m=\u001B[39m ravel(values)\n\u001B[1;32m   5443\u001B[0m     axis \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mndim\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m-> 5444\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mconcatenate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36mconcatenate\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 1000 and the array at index 1 has size 6"
     ]
    }
   ],
   "source": [
    "cleaned_zs_lists = cleanDatasetsIRQ(german_list)\n",
    "cleaned_km_list = cleanDatasetKMeans(german_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perform again the classification after outliers detection and print results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cleaned_DTscores_zs = computeScoresDT(cleaned_zs_lists)\n",
    "cleaned_NBscores_zs = computeScoresNB(cleaned_zs_lists)\n",
    "cleaned_DTscores_km = computeScoresDT(cleaned_km_list)\n",
    "cleaned_NBscores_km = computeScoresNB(cleaned_km_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "printScores(cleaned_DTscores_zs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "printScores(cleaned_NBscores_zs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "printScores(cleaned_DTscores_km)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "printScores(cleaned_NBscores_km)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot data preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transform the score object array into an array of test scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def toArray(score):\n",
    "    t = []\n",
    "    for s in score:\n",
    "        t.append(s['scores'][1])\n",
    "    return t"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preparation of plot DataFrame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plotData = {\n",
    "    'dirty': [0, 10, 20, 30, 40, 50],\n",
    "    'DTc_pre': toArray(DTscores),\n",
    "    'NB_pre': toArray(NBscore),\n",
    "    'DTc_zs_post': toArray(cleaned_DTscores_zs),\n",
    "    'NB_zs_post': toArray(cleaned_NBscores_zs),\n",
    "    'DTc_km_post': toArray(cleaned_DTscores_km),\n",
    "    'NB_km_post': toArray(cleaned_NBscores_km)\n",
    "}\n",
    "plotDF = pd.DataFrame(plotData)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Comparison between pre and post detection scores for each combination of algorithms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.set()\n",
    "fig, axes = plt.subplots(4, 1, sharex=False, figsize=(7,14))\n",
    "\n",
    "sns.lineplot(ax=axes[0], x='dirty', y='DTc_pre', data=plotDF)\n",
    "sns.lineplot(ax=axes[0], x='dirty', y='DTc_zs_post', data=plotDF)\n",
    "axes[0].legend(['Pre detection', 'Post detection'])\n",
    "axes[0].set(xlabel='Dirty percentage', ylabel='Performance')\n",
    "axes[0].set_ylim(0.2,1.1)\n",
    "axes[0].set_title('DTC + ZScore')\n",
    "\n",
    "sns.lineplot(ax=axes[1], x='dirty', y='NB_pre', data=plotDF)\n",
    "sns.lineplot(ax=axes[1], x='dirty', y='NB_zs_post', data=plotDF)\n",
    "axes[1].legend(['Pre detection', 'Post detection'])\n",
    "axes[1].set(xlabel='Dirty percentage', ylabel='Performance')\n",
    "axes[1].set_ylim(0.2,1.1)\n",
    "axes[1].set_title('NB + ZScore')\n",
    "\n",
    "sns.lineplot(ax=axes[2], x='dirty', y='DTc_pre', data=plotDF)\n",
    "sns.lineplot(ax=axes[2], x='dirty', y='DTc_km_post', data=plotDF)\n",
    "axes[2].legend(['Pre detection', 'Post detection'])\n",
    "axes[2].set(xlabel='Dirty percentage', ylabel='Performance')\n",
    "axes[2].set_ylim(0.2,1.1)\n",
    "axes[2].set_title('DTC + K-Means')\n",
    "\n",
    "sns.lineplot(ax=axes[3], x='dirty', y='NB_pre', data=plotDF)\n",
    "sns.lineplot(ax=axes[3], x='dirty', y='NB_km_post', data=plotDF)\n",
    "axes[3].legend(['Pre detection', 'Post detection'])\n",
    "axes[3].set(xlabel='Dirty percentage', ylabel='Performance')\n",
    "axes[3].set_ylim(0.2,1.1)\n",
    "axes[3].set_title('NB + K-Means')\n",
    "fig.tight_layout()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Comparison between the two classification algorithms pre-detection and post-detection (for each outlier detection algorithm)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.set()\n",
    "fig, axes = plt.subplots(3, 1, sharex=False, figsize=(7,18))\n",
    "\n",
    "sns.lineplot(ax=axes[0], x='dirty', y='DTc_pre', data=plotDF)\n",
    "sns.lineplot(ax=axes[0], x='dirty', y='NB_pre', data=plotDF)\n",
    "axes[0].legend(['Pre detection DTC', 'Pre detection NB'])\n",
    "axes[0].set(xlabel='Dirty percentage', ylabel='Performance')\n",
    "axes[0].set_ylim(0.2,1.1)\n",
    "axes[0].set_title('Pre detection performances')\n",
    "\n",
    "sns.lineplot(ax=axes[1], x='dirty', y='DTc_zs_post', data=plotDF)\n",
    "sns.lineplot(ax=axes[1], x='dirty', y='NB_zs_post', data=plotDF)\n",
    "axes[1].legend(['Post detection DTC', 'Post detection NB'])\n",
    "axes[1].set(xlabel='Dirty percentage', ylabel='Performance')\n",
    "axes[1].set_ylim(0.2,1.1)\n",
    "axes[1].set_title('Post detection (ZS) performances')\n",
    "\n",
    "sns.lineplot(ax=axes[2], x='dirty', y='DTc_km_post', data=plotDF)\n",
    "sns.lineplot(ax=axes[2], x='dirty', y='NB_km_post', data=plotDF)\n",
    "axes[2].legend(['Post detection DTC', 'Post detection NB'])\n",
    "axes[2].set(xlabel='Dirty percentage', ylabel='Performance')\n",
    "axes[2].set_ylim(0.2,1.1)\n",
    "axes[2].set_title('Post detection K-Means performances')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Comparison between detection algorithms for each post-detection one"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.set()\n",
    "fig, axes = plt.subplots(2, 1, sharex=False, figsize=(7,18))\n",
    "\n",
    "sns.lineplot(ax=axes[0], x='dirty', y='DTc_zs_post', data=plotDF)\n",
    "sns.lineplot(ax=axes[0], x='dirty', y='DTc_km_post', data=plotDF)\n",
    "axes[0].legend(['Z-Score', 'K-Means'])\n",
    "axes[0].set(xlabel='Dirty percentage', ylabel='Performance')\n",
    "axes[0].set_ylim(0.2,1.1)\n",
    "axes[0].set_title('DTC Post-detection performances')\n",
    "\n",
    "sns.lineplot(ax=axes[1], x='dirty', y='NB_zs_post', data=plotDF)\n",
    "sns.lineplot(ax=axes[1], x='dirty', y='NB_km_post', data=plotDF)\n",
    "axes[1].legend(['Z-Score', 'K-Means'])\n",
    "axes[1].set(xlabel='Dirty percentage', ylabel='Performance')\n",
    "axes[1].set_ylim(0.2,1.1)\n",
    "axes[1].set_title('NB Post-detection performances')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
